# -*- coding: utf-8 -*-
"""Kaggle_StudentGradesPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14X0U18RvkzEsx2F8xcwcNRMdbhlM2vKl

# Student Grades Prediction dataset taken from Kaggle
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import random
# %matplotlib inline

# Reading data
df = pd.read_csv('/content/drive/MyDrive/DataScience/Dataset/KAGGLE/StudentGrades/results.csv')

df.head()

# Shape of dataset
print(f'Rows:{df.shape[0]}', f'Columns:{df.shape[1]}')

# Removing first irrelevant column
df.drop(['Unnamed: 0'], axis=1, inplace=True)

# Now the last column in Division, which is of no use. It won't give any insights as comapred to others. Hence dropping it too.
df.drop(['Div'], axis=1, inplace=True)

"""# EDA"""

df.info()

df.describe()

df.isna().sum()

df.Results.value_counts()

"""## Defining the variables"""

X = df.drop(['Results'], axis=1)        # Independant Variable / Features
y = df['Results']                       # Dependant Variable / Target

"""## Getting important features"""

from sklearn.ensemble import ExtraTreesRegressor

model = ExtraTreesRegressor()
model.fit(X, y)

model.feature_importances_

"""## Visualizing Important Features"""

plt.figure(figsize=(12,8))
impft = pd.Series(model.feature_importances_, index=X.columns)
impft.nlargest(20).plot(kind='barh')
plt.show()

"""### As all the columns are numerical type, we don't need to do any OneHotEncoding or LabelEncoding

### Standard Scaling
"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

cols_to_scale = ['Hindi', 'English', 'Science', 'Maths', 'History', 'Geograpgy', 'Total']
df[cols_to_scale] = sc.fit_transform(df[cols_to_scale])

# Visualizing the correlation between the features

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='Reds')

"""# **BUILDING MODEL**"""

# Dividing the data into train and test
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)      # 80% train data, 20% test data

"""## **LOGISTIC REGRESSION**
---
"""

from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import LogisticRegression

log = LogisticRegression(random_state=42)

penalty = ['l1', 'l2', 'elasticnet', 'none']
class_weight = [None, 'balanced']
solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
max_iter = np.arange(100, 500, 100)
C = [100, 10, 1.0, 0.1, 0.01]
multi_class = ['auto', 'ovr', 'multinomial']

log_params = dict(penalty = penalty,
                  class_weight = class_weight,
                  solver = solver,
                  max_iter = max_iter,
                  C = C,
                  multi_class = multi_class)

"""### *Finding the best params*"""

mod_gs_log = GridSearchCV(estimator=log, param_grid=log_params, verbose=1, n_jobs=-1, cv=3)
mod_gs_log.fit(X_train, y_train)
bp_log = mod_gs_log.best_params_
bp_log

"""### *Training with best parameters*"""

mod_log = LogisticRegression(C=bp_log['C'], class_weight=bp_log['class_weight'], max_iter=bp_log['max_iter'], multi_class=bp_log['multi_class'], 
                             penalty=bp_log['penalty'], solver=bp_log['solver'])
mod_log.fit(X_train, y_train)

ypred_log = mod_log.predict(X_test)

# import metric libraries
from sklearn.metrics import mean_squared_error as mse
from math import sqrt as r

# Metrics for Logistic Regression

rmse_log = r(mse(y_test, ypred_log))
print('RMSE Log:', rmse_log)

"""## **SGD CLASSIFIER**
---


"""

from sklearn.linear_model import SGDClassifier

sgd = SGDClassifier(random_state=42)

loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']
penalty = ['l1', 'l2', 'elasticnet']
alpha = [0.001, 0.01, 0.1, 1, 10, 100]
learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']
# class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]
eta0 = [1, 10, 100]
max_iter = np.arange(200, 1000, 200)

sgd_params = dict(loss = loss,
                  penalty = penalty,
                  alpha = alpha,
                  learning_rate = learning_rate, 
                  class_weight = class_weight, 
                  eta0 = eta0,
                  max_iter = max_iter)

"""### *Finding the best params*"""

mod_gs_sgd = GridSearchCV(estimator=sgd, param_grid=sgd_params, verbose=1, n_jobs=-1, cv=3)
mod_gs_sgd.fit(X_train, y_train)
bp_sgd = mod_gs_sgd.best_params_
bp_sgd

"""### *Training with best parameters*"""

mod_sgd = SGDClassifier(loss=bp_sgd['loss'], penalty=bp_sgd['penalty'], max_iter=bp_sgd['max_iter'], learning_rate=bp_sgd['learning_rate'], eta0=bp_sgd['eta0'],
                        alpha=bp_sgd['alpha'])
mod_sgd.fit(X_train, y_train)

ypred_sgd = mod_sgd.predict(X_test)

# Metrics for SGD Classifier

rmse_sgd = r(mse(y_test, ypred_sgd))
print('RMSE SGD:', rmse_sgd)

"""## **DECISION TREE CLASSIFIER**
---
"""

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(random_state=42)

criterion = ['gini', 'entropy']
splitter = ['best', 'random']
max_depth = np.arange(1, 20, 2)
min_samples_split = np.arange(2, 10)
min_samples_leaf = np.arange(2, 10)
max_features = [None, 'auto', 'sqrt', 'log2']

dtc_params = dict(criterion = criterion,
                  splitter = splitter,
                  max_depth = max_depth,
                  min_samples_split = min_samples_split,
                  min_samples_leaf = min_samples_leaf,
                  max_features = max_features)

"""### *Finding the best params*"""

mod_gs_dtc = GridSearchCV(estimator = dtc, param_grid = dtc_params, verbose = 1, n_jobs = -1)
mod_gs_dtc.fit(X_train, y_train)
bp_dtc = mod_gs_dtc.best_params_
bp_dtc

"""### *Training with best parameters*"""

mod_dtc = DecisionTreeClassifier(max_depth=bp_dtc['max_depth'], min_samples_leaf=bp_dtc['min_samples_leaf'], criterion=bp_dtc['criterion'],
                                 max_features=bp_dtc['max_features'], min_samples_split=bp_dtc['min_samples_split'], splitter=bp_dtc['splitter'])
mod_dtc.fit(X_train, y_train)
ypred_dtc = mod_dtc.predict(X_test)

# Metrics for Decision Tree Classifier

rmse_dtc = r(mse(y_test, ypred_dtc))
print('RMSE DTC:', rmse_dtc)

"""## **RIDGE CLASSIFIER**
---
"""

from sklearn.linear_model import RidgeClassifier

rid = RidgeClassifier(random_state=42)

alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
solver = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
class_weight = ['balanced', None]
max_iter = np.arange(200, 2000, 200)

rid_params = dict(alpha = alpha,
                  max_iter = max_iter,
                  class_weight = class_weight,
                  solver = solver)

"""### *Finding the best params*"""

mod_gs_rid = GridSearchCV(estimator = rid, param_grid = rid_params, verbose = 1, n_jobs = -1)
mod_gs_rid.fit(X_train, y_train)
bp_rid = mod_gs_rid.best_params_
bp_rid

"""### *Training with best parameters*"""

mod_rid = RidgeClassifier(alpha=bp_rid['alpha'], class_weight=bp_rid['class_weight'], max_iter=bp_rid['max_iter'], solver=bp_rid['solver'])

mod_rid.fit(X_train, y_train)
ypred_rid = mod_rid.predict(X_test)

# Metrics for Ridge Classifier

rmse_rid = r(mse(y_test, ypred_rid))
print('RMSE RID:', rmse_rid)

"""## **K-Nearest Neighbors (KNN)**
---
"""

from sklearn.neighbors import KNeighborsClassifier

knc = KNeighborsClassifier()

n_neighbors = np.arange(1, 21, 2)
weights = ['uniform', 'distance']
algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']
leaf_size = np.arange(0, 50, 5)
metric = ['euclidean', 'manhattan', 'minkowski']

knc_params = dict(n_neighbors = n_neighbors,
                  weights = weights,
                  algorithm = algorithm,
                  leaf_size = leaf_size,
                  metric = metric)

"""### *Finding the best params*"""

mod_gs_knc = GridSearchCV(estimator = knc, param_grid = knc_params, verbose = 1, n_jobs = -1)
mod_gs_knc.fit(X_train, y_train)
bp_knc = mod_gs_knc.best_params_
bp_knc

"""### *Training with best params*"""

mod_knc = KNeighborsClassifier(algorithm=bp_knc['algorithm'], leaf_size=bp_knc['leaf_size'], metric=bp_knc['metric'], n_neighbors=bp_knc['n_neighbors'],
                          weights=bp_knc['weights'])

mod_knc.fit(X_train, y_train)
ypred_knc = mod_knc.predict(X_test)

# Metrics for KNeighbors Classifier

rmse_knc = r(mse(y_test, ypred_knc))
print('RMSE KNC:', rmse_knc)

"""## **RANDOM FOREST**
---
"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(random_state=42)

n_estimators = [2, 4, 8, 16, 32, 64, 100]
max_features = ['sqrt', 'log2', 'auto']
criterion = ['gini', 'entropy']
max_depth = np.arange(1, 16, 2)
min_samples_split = [0.01, 0.01, 0.1, 1.0]
min_samples_leaf = [0.01, 0.01, 0.1, 1.0]
class_weight = ['balanced', 'balanced_subsample']

rfc_params = dict(n_estimators = n_estimators,
                  max_features = max_features,
                  criterion = criterion,
                  max_depth = max_depth,
                  min_samples_split = min_samples_split,
                  min_samples_leaf = min_samples_leaf,
                  class_weight = class_weight)

"""### *Finding the best params*"""

mod_gs_rfc = GridSearchCV(estimator = rfc, param_grid = rfc_params, verbose = 1, n_jobs = -1)
mod_gs_rfc.fit(X_train, y_train)
bp_rfc = mod_gs_rfc.best_params_
bp_rfc

"""### *Training with best params*"""

mod_rfc = RandomForestClassifier(n_estimators=bp_rfc['n_estimators'], max_features=bp_rfc['max_features'], criterion=bp_rfc['criterion'],
                  max_depth=bp_rfc['max_depth'], min_samples_split=bp_rfc['min_samples_split'], min_samples_leaf=bp_rfc['min_samples_leaf'],
                  class_weight=bp_rfc['class_weight'])

mod_rfc.fit(X_train, y_train)
ypred_rfc = mod_rfc.predict(X_test)

# Metrics for Random Forest Classifier

rmse_rfc = r(mse(y_test, ypred_rfc))
print('RMSE RFC:', rmse_rfc)

"""## **ADA-BOOST CLASSIFIER**
---
"""

from sklearn.ensemble import AdaBoostClassifier

abc = AdaBoostClassifier(random_state=42)

algorithm = ['SAMME', 'SAMME.R']
n_estimators = [10, 50, 100, 200]
learning_rate = [10, 1, 0.1, 0.01, 0.001]

abc_params = dict(algorithm = algorithm,
                 n_estimators = n_estimators,
                 learning_rate = learning_rate)

"""### *Finding the best params*"""

mod_gs_abc = GridSearchCV(estimator = abc, param_grid = abc_params, verbose = 1, n_jobs = -1)
mod_gs_abc.fit(X_train, y_train)
bp_abc = mod_gs_abc.best_params_
bp_abc

"""### *Training with best params*"""

mod_abc = AdaBoostClassifier(algorithm=bp_abc['algorithm'], n_estimators=bp_abc['n_estimators'], learning_rate=0.1)

mod_abc.fit(X_train, y_train)
ypred_abc = mod_abc.predict(X_test)

# Metrics for Ada Boost Classifier

rmse_abc = r(mse(y_test, ypred_abc))
print('RMSE ABC:', rmse_abc)

"""## **SUPPORT VECTOR CLASSIFIER**
---

#### *This is not working properly on my machine, so I'll skip SVC*
"""

# from sklearn.svm import SVC

# svc = SVC(random_state=42)

# C = [100, 10, 1.0, 0.1, 0.01]
# kernel = ['linear', 'poly', 'rbf', 'sigmoid']
# gamma = ['scale', 'auto']
# # decision_function_shape = ['ovo', 'ovr']

# svc_params = dict(C = C,
#                   kernel = kernel,
#                   gamma = gamma)
#                   # decision_function_shape = decision_function_shape)

"""### *Finding the best params*"""

# mod_gs_svc = GridSearchCV(estimator = svc, param_grid = svc_params, verbose = 1, n_jobs = -1, cv=3)
# mod_gs_svc.fit(X_train, y_train)
# bp_svc = mod_gs_svc.best_params_
# bp_svc

"""### *Training with best params*"""

# mod_svc = SVC(C=bp_svc['C'], kernel=bp_svc['kernel'], gamma=bp_svc['gamma'])
# # , decision_funtion_shape=bp_svc['decision_function_shape'])

# mod_svc.fit(X_train, y_train)
# ypred_svc = mod_svc.predict(X_test)

# # Metrics for Support Vaector Classifier

# rmse_svc = r(mse(y_test, ypred_svc))
# print('RMSE SVC:', rmse_svc)

"""## **CATEGORICAL NAIVE BAYES**
---
"""

from sklearn.naive_bayes import CategoricalNB

mod_cnb = CategoricalNB()

mod_cnb.fit(X_train, y_train)
ypred_cnb = mod_cnb.predict(X_test)

# Metrics for Categorical Naive Bayes

rmse_cnb = r(mse(y_test, ypred_cnb))
print('RMSE CNB:', rmse_cnb)

"""## Comparing all the RMSE values:"""

print('RMSE ABC:', rmse_abc)
print('RMSE CNB:', rmse_cnb)
print('RMSE DTC:', rmse_dtc)
print('RMSE KNC:', rmse_knc)
print('RMSE LOG:', rmse_log)
print('RMSE RFC:', rmse_rfc)
print('RMSE RID:', rmse_rid)
print('RMSE SGD:', rmse_sgd)

"""### As seen here, the Random Forest has the lowest RMSE value. Hence we will use it for prediction the output."""

# Saving the RFC model
import pickle

rfc_model = 'rfc_model.sav'
pickle.dump(mod_rfc, open(rfc_model, 'wb'))

"""# **TEST DATA**
---
"""

# Loading the saved RFC model
rfc_loaded = pickle.load(open(rfc_model, 'rb'))

# Testing on the Test Dataset
test_model = rfc_loaded.fit(X_train, y_train)
test_ypred = test_model.predict(X_test)

# Metrics for final output

rmse_output = r(mse(y_test, test_ypred))
print('RMSE Output:', rmse_output)

# Accuracy score metrics
from sklearn.metrics import accuracy_score

accuracy_score(y_test, test_ypred)

"""As we used the best model Random Forest Classifier, we can conclude that ***0.92 accuracy*** and ***RMSE 0.282842*** is the best score we can get by using these models. However note that by tweaking the parameters more can result in different outputs."""